{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SENTIMENT ANALYSIS\n",
    "\n",
    "#Dataset link\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('../Datasets'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-18151064f0f42efa\n",
      "Found cached dataset json (/Users/test/.cache/huggingface/datasets/json/default-18151064f0f42efa/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eed5fcc16524e87b596e8748e9c9eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_v2_path = \"../Datasets/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "df = pd.read_json(dataset_v2_path, lines=True)\n",
    "#print(df.head())\n",
    "\n",
    "dataset_hf=load_dataset(\"json\", data_files=dataset_v2_path)\n",
    "dataset_hf=dataset_hf.remove_columns(['article_link'])\n",
    "dataset_hf.set_format('pandas')\n",
    "dataset_hf=dataset_hf['train'][:]\n",
    "dataset_hf.drop_duplicates(subset=['headline'], inplace=True)\n",
    "dataset_hf=dataset_hf.reset_index()[['headline', 'label']]\n",
    "\n",
    "dataset_hf=Dataset.from_pandas(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['headline', 'label'], 'test': ['headline', 'label']}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 22802\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 2851\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 2850\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Train Test Valid Split - scikit\n",
    "train_testvalid = dataset_hf.train_test_split(test_size=0.2,seed=15)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
    "dataset_hf = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 22802\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 2851\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['headline', 'label'],\n",
      "        num_rows: 2850\n",
      "    })\n",
      "})\n",
      "{'headline': \"dan harmon finally reveals reason behind 'rick and morty' delays\", 'label': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed289095114431b2123800cae8fc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c698aea9824f50bf908af07c8f6050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7748297ea8496b8e22a21487994e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['headline', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 22802\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['headline', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2851\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['headline', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2850\n",
      "    })\n",
      "})\n",
      "{'label': tensor(0), 'input_ids': tensor([  101,  4907, 25546,  2633,  7657,  3114,  2369,  1005,  6174,  1998,\n",
      "        22294,  2100,  1005, 14350,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "#Checkpoint\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "#In the model distilbert-base-uncased\n",
    "# #each token is embedded into a vector of size 768. \n",
    "# The shape of the output from the base model is\n",
    "# (batch_size, max_sequence_length, embedding_vector_size=768)\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_len = 512\n",
    "\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"headline\"], truncation=True, max_length=512)\n",
    "\n",
    "print(dataset_hf)\n",
    "print(dataset_hf['train'][0])\n",
    "tokenized_dataset = dataset_hf.map(tokenize, batched=True) #apply tokenize to each row\n",
    "print(tokenized_dataset)\n",
    "tokenized_dataset.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"label\"] )\n",
    "print(tokenized_dataset['train'][0])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "# Data collators are objects that will form a batch by using a list of dataset elements as input. \n",
    "# These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "# To be able to build batches, data collators may apply some processing (like padding). \n",
    "#  Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation \n",
    "# (like random masking) on the formed batch.\n",
    "# data_collator automatically pads the model inputs in a batch to the length of the longest example.\n",
    "# This bypasses the need to set a global maximum sequence length, \n",
    "# and in practice leads to faster training since we perform fewer redundant computations \n",
    "# on the padded tokens and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTaskSpecificCustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels ):\n",
    "        super(MyTaskSpecificCustomModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.model = model = AutoModel.from_pretrained(checkpoint, config = AutoConfig.from_pretrained(checkpoint, \n",
    "                                                                                                       output_attention = True, \n",
    "                                                                                                       output_hidden_state = True ) )\n",
    "        # New Layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels )\n",
    "        \n",
    "    def forward(self, input_ids = None, attention_mask=None, labels = None ):\n",
    "        #If the attention_mask is 0, the token id is ignored. \n",
    "        # For instance if a sequence is padded to adjust the sequence length, \n",
    "        # the padded words should be ignored hence their attention_mask are 0.\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask = attention_mask  )\n",
    "        \n",
    "        last_hidden_state = outputs[0]\n",
    "        \n",
    "        sequence_outputs = self.dropout(last_hidden_state)\n",
    "        \n",
    "        logits = self.classifier(sequence_outputs[:, 0, : ].view(-1, 768 )) #batch_size by 768\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "            return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sense of nn.Linear\n",
    "In your Neural Network, the self.hidden = nn.Linear(784, 256) defines a hidden (meaning that it is in between of the input and output layers), fully connected linear layer, \n",
    "which takes input x of shape (batch_size, 784), where batch size is the number of inputs (each of size 784) which are passed to the network at once (as a single tensor), \n",
    "and transforms it by the linear equation y = x*W^T + b into a tensor y of shape (batch_size, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch Data Loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'], shuffle = True, batch_size = 32, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset['valid'], shuffle = True, collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/test/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee756159e434baca756275b7ac1b8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23250e88585c405d9623c3e452e2a5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9067830226100754}\n",
      "{'f1': 0.92913982569155}\n",
      "{'f1': 0.9282442748091603}\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_task_specific = MyTaskSpecificCustomModel(checkpoint=checkpoint, num_labels=2 ).to(device)\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model_task_specific.parameters(), lr = 5e-5 )\n",
    "\n",
    "num_epoch = 3\n",
    "\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    "    \n",
    ")\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epoch * len(eval_dataloader) ))\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model_task_specific.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        #print(batch)\n",
    "        outputs = model_task_specific(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "        \n",
    "    model_task_specific.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        with torch.no_grad():\n",
    "            outputs = model_task_specific(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim = -1 )\n",
    "        metric.add_batch(predictions = predictions, references = batch['labels'] )\n",
    "        progress_bar_eval.update(1)\n",
    "        \n",
    "    print(metric.compute()) \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9285449833395039}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Post training evaluation\n",
    "model_task_specific.eval()\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset['test'], batch_size = 32, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = { k: v.to(device) for k, v in batch.items() }\n",
    "    with torch.no_grad():\n",
    "        outputs = model_task_specific(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim = -1)\n",
    "    metric.add_batch(predictions = predictions, references=batch['labels'] )\n",
    "    \n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "521bf23a49300457a383cc0ce4a9a5b8cdf2cad9d8aaec6ddd3bd1c99845bf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
