{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "sentence = 'She is a Machine Learning Engineer and works at Silicon Valley in California.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'is', 'a', 'machine', 'learning', 'engineer', 'and', 'works', 'at', 'silicon', 'valley', 'in', 'california', '.']\n",
      "['[CLS]', 'she', 'is', 'a', 'machine', 'learning', 'engineer', 'and', 'works', 'at', 'silicon', 'valley', 'in', 'california', '.', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]'] #start and end tokens\n",
    "tokens = tokens + ['[PAD]'] + ['[PAD]'] #how to add padding\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Attention Mask\n",
    "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2016, 2003, 1037, 3698, 4083, 3992, 1998, 2573, 2012, 13773, 3028, 1999, 2662, 1012, 102, 0, 0]\n",
      "tensor([[  101,  2016,  2003,  1037,  3698,  4083,  3992,  1998,  2573,  2012,\n",
      "         13773,  3028,  1999,  2662,  1012,   102,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#Unique Token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "print(token_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1099,  0.0468, -0.6595,  ..., -0.2623,  0.5234,  0.2463],\n",
      "         [ 0.2299, -0.4289, -0.8433,  ...,  0.1221,  0.3654, -0.2318],\n",
      "         [-0.1189,  0.1354, -0.0965,  ..., -0.4777,  0.1248,  0.2993],\n",
      "         ...,\n",
      "         [-0.0959,  0.2789, -0.8420,  ...,  0.3402, -0.3266, -0.5629],\n",
      "         [ 0.0823,  0.2564, -0.2476,  ...,  0.3251,  0.2154, -0.1537],\n",
      "         [ 0.0115,  0.2023, -0.3834,  ...,  0.4589,  0.3258, -0.4677]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 18, 768])\n",
      "\n",
      "\n",
      "\n",
      "tensor([[-9.5204e-01, -5.6682e-01, -9.3476e-01,  9.0912e-01,  8.6666e-01,\n",
      "         -3.3698e-01,  9.4771e-01,  5.7378e-01, -8.6329e-01, -1.0000e+00,\n",
      "         -7.5686e-01,  9.6853e-01,  9.8878e-01,  5.4817e-01,  9.6468e-01,\n",
      "         -8.5410e-01, -2.4779e-01, -7.3305e-01,  4.6576e-01, -7.4243e-01,\n",
      "          8.0826e-01,  1.0000e+00, -5.6346e-02,  4.6798e-01,  6.2356e-01,\n",
      "          9.9699e-01, -8.7362e-01,  9.6176e-01,  9.7230e-01,  8.6774e-01,\n",
      "         -8.1800e-01,  4.6473e-01, -9.9430e-01, -3.3792e-01, -9.3962e-01,\n",
      "         -9.9631e-01,  6.3671e-01, -8.6220e-01, -8.0238e-02, -1.8155e-01,\n",
      "         -9.2166e-01,  5.7115e-01,  1.0000e+00, -1.5062e-01,  5.8745e-01,\n",
      "         -3.2634e-01, -1.0000e+00,  4.2109e-01, -9.5622e-01,  9.5597e-01,\n",
      "          8.6567e-01,  9.3213e-01,  3.2232e-01,  6.8538e-01,  6.0227e-01,\n",
      "         -4.1709e-01,  3.4451e-02,  2.1924e-01, -3.7729e-01, -8.0239e-01,\n",
      "         -6.7119e-01,  4.6067e-01, -8.9792e-01, -9.5884e-01,  9.4615e-01,\n",
      "          8.3210e-01, -3.3805e-01, -3.8451e-01, -2.6452e-01, -4.4347e-02,\n",
      "          9.7518e-01,  4.0303e-01, -6.4003e-02, -8.9326e-01,  8.0512e-01,\n",
      "          3.3236e-01, -7.6712e-01,  1.0000e+00, -5.9582e-01, -9.9010e-01,\n",
      "          8.5198e-01,  8.4972e-01,  7.5364e-01, -3.5678e-01,  5.7969e-01,\n",
      "         -1.0000e+00,  6.8442e-01, -2.0930e-01, -9.9583e-01,  3.2225e-01,\n",
      "          6.3482e-01, -3.5431e-01,  5.0968e-01,  7.6813e-01, -4.8295e-01,\n",
      "         -6.8469e-01, -4.9585e-01, -9.0498e-01, -4.6612e-01, -4.9699e-01,\n",
      "          2.1777e-01, -4.0617e-01, -5.0448e-01, -5.3622e-01,  4.1061e-01,\n",
      "         -6.7157e-01, -6.6086e-01,  5.3451e-01, -1.3891e-02,  7.2763e-01,\n",
      "          6.3556e-01, -4.8320e-01,  5.7396e-01, -9.7081e-01,  7.5515e-01,\n",
      "         -3.7761e-01, -9.9334e-01, -7.7092e-01, -9.9559e-01,  8.4366e-01,\n",
      "         -4.5507e-01, -4.0910e-01,  9.8529e-01,  5.4373e-03,  6.2369e-01,\n",
      "         -3.4076e-01, -9.6578e-01, -1.0000e+00, -7.3211e-01, -1.4012e-01,\n",
      "         -2.6132e-01, -3.3205e-01, -9.8984e-01, -9.7365e-01,  7.2977e-01,\n",
      "          9.6994e-01,  4.2376e-01,  9.9999e-01, -6.2378e-01,  9.6046e-01,\n",
      "         -4.7777e-01, -8.7974e-01,  7.6007e-01, -6.1590e-01,  7.4659e-01,\n",
      "          4.8530e-01, -7.7476e-01,  3.8284e-01, -6.4347e-01,  3.7741e-01,\n",
      "         -8.5363e-01, -4.6510e-01, -6.7202e-01, -9.6232e-01, -4.6965e-01,\n",
      "          9.7122e-01, -7.7407e-01, -9.6941e-01, -1.2454e-01, -5.0831e-01,\n",
      "         -5.9482e-01,  9.0603e-01,  8.3329e-01,  5.2977e-01, -4.6481e-01,\n",
      "          6.1236e-01,  2.9392e-01,  7.1893e-01, -9.0229e-01, -4.4198e-01,\n",
      "          5.7424e-01, -5.6244e-01, -8.7476e-01, -9.9067e-01, -5.7931e-01,\n",
      "          7.5022e-01,  9.9369e-01,  8.5855e-01,  4.0036e-01,  9.0372e-01,\n",
      "         -4.9284e-01,  8.3559e-01, -9.7896e-01,  9.9187e-01, -3.5423e-01,\n",
      "          3.8222e-01, -2.6170e-01,  4.5815e-01, -9.0084e-01, -5.1448e-02,\n",
      "          9.1587e-01, -8.4910e-01, -9.3583e-01, -1.8606e-01, -5.5720e-01,\n",
      "         -5.9432e-01, -8.5330e-01,  7.3645e-01, -4.9013e-01, -5.3596e-01,\n",
      "         -2.9126e-01,  9.5795e-01,  9.9441e-01,  8.2772e-01,  3.5841e-01,\n",
      "          7.9474e-01, -9.7108e-01, -6.0369e-01,  2.4691e-01,  4.1386e-01,\n",
      "          3.5754e-01,  9.9575e-01, -7.0311e-01, -3.8314e-01, -9.4951e-01,\n",
      "         -9.9057e-01,  8.2268e-02, -9.5715e-01, -2.3830e-01, -7.9260e-01,\n",
      "          8.0145e-01, -7.5075e-01,  6.7125e-01,  5.7751e-01, -9.9489e-01,\n",
      "         -8.4702e-01,  4.9907e-01, -5.9386e-01,  6.3220e-01, -2.9725e-01,\n",
      "          6.5900e-01,  9.5177e-01, -7.8221e-01,  6.6698e-01,  9.4921e-01,\n",
      "         -9.0537e-01, -8.9844e-01,  9.1504e-01, -4.2252e-01,  9.2854e-01,\n",
      "         -8.3376e-01,  9.9868e-01,  9.3425e-01,  8.3378e-01, -9.7478e-01,\n",
      "         -7.4943e-01, -9.5908e-01, -7.6674e-01, -1.9424e-01,  1.6134e-01,\n",
      "          9.2963e-01,  7.9686e-01,  5.7064e-01,  4.3692e-01, -6.9250e-01,\n",
      "          9.9959e-01, -8.5431e-01, -9.6797e-01, -3.7911e-01, -4.9534e-01,\n",
      "         -9.9198e-01,  8.9661e-01,  4.4446e-01,  5.6019e-01, -6.3172e-01,\n",
      "         -7.9941e-01, -9.7283e-01,  9.5340e-01,  3.4142e-01,  9.9792e-01,\n",
      "         -4.5422e-01, -9.6383e-01, -7.8351e-01, -9.6390e-01,  2.0665e-02,\n",
      "         -2.3595e-01, -4.9863e-01,  5.3966e-02, -9.8051e-01,  6.4595e-01,\n",
      "          7.1503e-01,  7.7420e-01, -9.2403e-01,  9.9973e-01,  1.0000e+00,\n",
      "          9.8304e-01,  9.4972e-01,  9.6133e-01, -9.9998e-01, -7.3335e-01,\n",
      "          1.0000e+00, -9.9724e-01, -1.0000e+00, -9.7420e-01, -7.4106e-01,\n",
      "          4.6007e-01, -1.0000e+00, -2.6046e-01, -4.0126e-02, -9.4139e-01,\n",
      "          7.7997e-01,  9.8604e-01,  9.9861e-01, -1.0000e+00,  9.5589e-01,\n",
      "          9.7859e-01, -7.8941e-01,  9.7220e-01, -4.7679e-01,  9.8388e-01,\n",
      "          5.6528e-01,  7.3770e-01, -5.0256e-01,  5.8802e-01, -9.4634e-01,\n",
      "         -9.2350e-01, -7.6962e-01, -8.8347e-01,  9.9947e-01,  2.3535e-01,\n",
      "         -8.9672e-01, -9.5356e-01,  7.2359e-01, -3.8466e-02, -2.1831e-01,\n",
      "         -9.7994e-01, -4.5380e-01,  7.4138e-01,  7.9205e-01,  3.7349e-01,\n",
      "          4.9056e-01, -8.0717e-01,  4.2284e-01, -1.2726e-01,  5.3324e-01,\n",
      "          7.7814e-01, -9.5226e-01, -7.8626e-01, -7.3740e-02,  1.0268e-01,\n",
      "         -6.5360e-01, -9.7697e-01,  9.8526e-01, -6.8219e-01,  9.0486e-01,\n",
      "          1.0000e+00,  3.3187e-01, -9.6861e-01,  7.6513e-01,  4.5046e-01,\n",
      "          2.3336e-01,  1.0000e+00,  8.4777e-01, -9.9065e-01, -7.1960e-01,\n",
      "          8.2179e-01, -7.6615e-01, -8.0606e-01,  9.9994e-01, -4.1495e-01,\n",
      "         -7.4241e-01, -6.3852e-01,  9.8918e-01, -9.9277e-01,  9.9807e-01,\n",
      "         -9.4457e-01, -9.8200e-01,  9.8582e-01,  9.6777e-01, -6.2400e-01,\n",
      "         -8.0749e-01,  1.9521e-01, -7.2141e-01,  4.0489e-01, -9.7879e-01,\n",
      "          8.2818e-01,  5.8865e-01, -2.2449e-01,  9.2657e-01, -8.5503e-01,\n",
      "         -6.9279e-01,  4.7425e-01, -5.9813e-01, -2.0674e-01,  9.5960e-01,\n",
      "          6.5583e-01, -3.3626e-01,  2.2022e-01, -4.9482e-01, -2.5752e-01,\n",
      "         -9.8676e-01,  6.1112e-01,  1.0000e+00, -2.2703e-01,  7.5682e-01,\n",
      "         -4.5582e-01, -2.6918e-01,  1.5254e-01,  6.7971e-01,  7.0268e-01,\n",
      "         -4.2133e-01, -9.4394e-01,  7.6520e-01, -9.8674e-01, -9.9469e-01,\n",
      "          8.4004e-01,  3.0061e-01, -3.8184e-01,  1.0000e+00,  5.3800e-01,\n",
      "          3.9185e-01,  4.1402e-01,  9.9608e-01, -4.0997e-04,  6.5973e-01,\n",
      "          8.8860e-01,  9.8979e-01, -3.9766e-01,  7.5889e-01,  9.3686e-01,\n",
      "         -9.4085e-01, -4.7340e-01, -7.8438e-01,  8.3587e-02, -9.1856e-01,\n",
      "         -5.9158e-02, -9.8682e-01,  9.8644e-01,  9.7603e-01,  4.9585e-01,\n",
      "          4.2288e-01,  6.9232e-01,  1.0000e+00, -7.6421e-01,  7.6375e-01,\n",
      "         -4.3355e-01,  7.9874e-01, -9.9994e-01, -9.3761e-01, -5.4206e-01,\n",
      "         -9.9736e-02, -8.2283e-01, -3.8275e-01,  4.2346e-01, -9.8838e-01,\n",
      "          8.2156e-01,  8.2344e-01, -9.9307e-01, -9.9284e-01, -3.9053e-01,\n",
      "          9.3720e-01,  3.4483e-01, -9.9410e-01, -8.4370e-01, -6.6755e-01,\n",
      "          8.1637e-01, -3.8483e-01, -9.7003e-01, -2.1267e-01, -4.6617e-01,\n",
      "          7.0971e-01, -4.7760e-01,  6.9896e-01,  8.9255e-01,  7.7753e-01,\n",
      "         -7.9859e-01, -6.1513e-01, -3.5410e-01, -8.9034e-01,  8.4365e-01,\n",
      "         -9.2697e-01, -9.7750e-01, -3.3032e-01,  1.0000e+00, -4.3216e-01,\n",
      "          8.6202e-01,  7.9489e-01,  8.5131e-01, -2.9219e-01,  3.6364e-01,\n",
      "          9.6948e-01,  3.3731e-01, -7.6945e-01, -9.2497e-01, -7.3085e-01,\n",
      "         -5.1925e-01,  6.3822e-01,  4.5559e-01,  6.8581e-01,  8.6736e-01,\n",
      "          8.3800e-01,  4.7331e-01, -1.1063e-01,  2.5572e-01,  9.9994e-01,\n",
      "         -2.1549e-01, -3.1244e-01, -8.0972e-01, -2.7024e-01, -5.6376e-01,\n",
      "         -2.7692e-01,  1.0000e+00,  5.0486e-01,  6.9138e-01, -9.9545e-01,\n",
      "         -9.1884e-01, -9.8036e-01,  1.0000e+00,  9.1601e-01, -9.3905e-01,\n",
      "          8.2882e-01,  7.2932e-01, -3.6087e-01,  8.2233e-01, -4.8587e-01,\n",
      "         -4.4129e-01,  3.2858e-01,  2.0040e-01,  9.8275e-01, -6.7214e-01,\n",
      "         -9.8460e-01, -7.2099e-01,  6.9307e-01, -9.8346e-01,  9.9999e-01,\n",
      "         -7.9120e-01, -4.2877e-01, -5.2764e-01, -6.4840e-01,  5.0701e-01,\n",
      "          4.8426e-02, -9.9212e-01, -4.6906e-01,  3.3393e-01,  9.8849e-01,\n",
      "          4.6734e-01, -7.4158e-01, -9.5904e-01,  9.3900e-01,  8.1261e-01,\n",
      "         -9.4547e-01, -9.6801e-01,  9.8079e-01, -9.8957e-01,  6.3945e-01,\n",
      "          1.0000e+00,  4.1992e-01,  3.1416e-01,  3.9132e-01, -5.5140e-01,\n",
      "          5.4256e-01, -5.4754e-01,  7.6520e-01, -9.7686e-01, -5.4543e-01,\n",
      "         -3.4484e-01,  5.7300e-01, -2.9077e-01, -5.0612e-01,  7.5502e-01,\n",
      "          3.5399e-01, -7.2044e-01, -7.3753e-01, -4.0416e-01,  5.8784e-01,\n",
      "          9.0515e-01, -3.8782e-01, -2.6179e-01,  2.2779e-01, -1.8261e-01,\n",
      "         -9.7995e-01, -6.0958e-01, -6.3644e-01, -1.0000e+00,  8.8311e-01,\n",
      "         -1.0000e+00,  7.2273e-01,  2.8160e-01, -3.7080e-01,  9.2266e-01,\n",
      "          6.8113e-01,  7.4754e-01, -8.5790e-01, -9.0600e-01,  5.2210e-01,\n",
      "          8.4504e-01, -6.0091e-01, -7.0058e-01, -8.2148e-01,  5.9790e-01,\n",
      "         -2.5207e-01,  4.7822e-01, -7.1896e-01,  8.0524e-01, -5.1378e-01,\n",
      "          1.0000e+00,  2.8019e-01, -6.7656e-01, -9.9096e-01,  4.4896e-01,\n",
      "         -4.0279e-01,  1.0000e+00, -9.5278e-01, -9.7832e-01,  4.7717e-01,\n",
      "         -8.8979e-01, -9.1326e-01,  4.9777e-01,  1.0996e-01, -8.9241e-01,\n",
      "         -9.7635e-01,  9.7935e-01,  9.5101e-01, -7.3177e-01,  6.3104e-01,\n",
      "         -4.6561e-01, -6.9690e-01,  9.9309e-02,  9.6047e-01,  9.9361e-01,\n",
      "          2.8915e-01,  9.4951e-01, -6.4227e-01, -4.8018e-02,  9.8433e-01,\n",
      "          2.9564e-01,  7.1634e-01,  3.1270e-01,  1.0000e+00,  5.1193e-01,\n",
      "         -9.4343e-01,  1.2887e-01, -9.9279e-01, -3.4794e-01, -9.6435e-01,\n",
      "          5.5505e-01,  3.0024e-01,  9.5590e-01, -4.3236e-01,  9.8309e-01,\n",
      "         -9.3922e-01,  2.0125e-01, -7.5226e-01, -5.4356e-01,  4.7448e-01,\n",
      "         -9.7724e-01, -9.8984e-01, -9.9050e-01,  7.5784e-01, -5.3063e-01,\n",
      "         -3.0203e-01,  3.9968e-01,  2.5153e-01,  6.8588e-01,  6.5987e-01,\n",
      "         -1.0000e+00,  9.7401e-01,  6.4351e-01,  9.4801e-01,  9.8364e-01,\n",
      "          7.2643e-01,  6.6574e-01,  4.4974e-01, -9.9216e-01, -9.9473e-01,\n",
      "         -5.0978e-01, -3.2251e-01,  7.9280e-01,  8.4534e-01,  9.0579e-01,\n",
      "          5.3879e-01, -5.8577e-01, -6.4860e-01, -4.9061e-01, -8.8398e-01,\n",
      "         -9.9617e-01,  6.9410e-01, -7.5589e-01, -9.8877e-01,  9.7679e-01,\n",
      "         -3.1878e-01, -1.6968e-01,  5.6594e-02, -7.9821e-01,  9.8550e-01,\n",
      "          9.0224e-01,  4.9888e-01,  2.5002e-01,  6.7521e-01,  9.5558e-01,\n",
      "          9.6891e-01,  9.9051e-01, -9.1233e-01,  9.2981e-01, -9.2373e-01,\n",
      "          5.5359e-01,  5.7639e-01, -9.6735e-01,  3.0182e-01,  7.6651e-01,\n",
      "         -5.0940e-01,  4.1806e-01, -3.4525e-01, -9.8929e-01,  6.6385e-01,\n",
      "         -3.8190e-01,  6.5570e-01, -6.3867e-01, -5.7040e-02, -5.4497e-01,\n",
      "         -2.6208e-01, -8.0532e-01, -7.9596e-01,  7.7287e-01,  7.0965e-01,\n",
      "          9.5693e-01,  9.0317e-01, -3.0217e-01, -8.5371e-01, -2.6544e-01,\n",
      "         -8.0463e-01, -9.4310e-01,  9.6012e-01, -1.4275e-01, -2.5081e-01,\n",
      "          7.2989e-01,  2.4797e-02,  8.9456e-01,  1.0418e-01, -5.3406e-01,\n",
      "         -5.7193e-01, -8.5518e-01,  9.5416e-01, -7.7291e-01, -7.3280e-01,\n",
      "         -6.0763e-01,  8.6990e-01,  5.1392e-01,  1.0000e+00, -8.5930e-01,\n",
      "         -9.2025e-01, -6.7247e-01, -5.9097e-01,  5.3436e-01, -6.1691e-01,\n",
      "         -1.0000e+00,  5.2481e-01, -8.1533e-01,  6.5873e-01, -6.8991e-01,\n",
      "          8.9630e-01, -8.7671e-01, -9.8947e-01, -4.1930e-01,  5.0148e-01,\n",
      "          8.8309e-01, -5.9721e-01, -8.2183e-01,  7.5821e-01, -4.1341e-01,\n",
      "          9.9201e-01,  9.2106e-01, -8.0649e-01,  5.3377e-02,  7.9797e-01,\n",
      "         -5.1580e-01, -7.2688e-01,  9.4257e-01]], grad_fn=<TanhBackward0>)\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#Get embedding\n",
    "output = model(token_ids, attention_mask = attention_mask)\n",
    "\n",
    "#last_hidden_state\n",
    "print(output[0])\n",
    "print(output[0].shape) #[1,18,768] #(1, num words, embed_dim)\n",
    "print(\"\\n\\n\")\n",
    "#pooler_output\n",
    "print(output[1])\n",
    "print(output[1].shape) #[1, 768]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "521bf23a49300457a383cc0ce4a9a5b8cdf2cad9d8aaec6ddd3bd1c99845bf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
