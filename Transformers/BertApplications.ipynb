{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masked Language modelling : perform word prediction\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = '2023 will be a great year for all of us'\n",
    "encoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
    "#print(encoding) #dictionary including input_ids, attention_mask, and token_type_ids\n",
    "input = encoding[\"input_ids\"][0]\n",
    "attention_mask = encoding[\"attention_mask\"][0]\n",
    "#tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Opera House in Australia is in , sydney city\n",
      "The Opera House in Australia is in , melbourne city\n",
      "The Opera House in Australia is in , brisbane city\n",
      "The Opera House in Australia is in , adelaide city\n",
      "The Opera House in Australia is in , the city\n",
      "The Opera House in Australia is in , canberra city\n",
      "The Opera House in Australia is in , auckland city\n",
      "The Opera House in Australia is in , hobart city\n",
      "The Opera House in Australia is in , griffith city\n",
      "The Opera House in Australia is in , hume city\n",
      "sydney\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)\n",
    "\n",
    "''' Masked Language Modeling works by inserting a mask token at the desired position\n",
    " where you want to predict the best candidate word that would go in that position.\n",
    "\n",
    "You can simply insert the mask token by concatenating it at the desired position\n",
    "\n",
    "The Bert Model for Masked Language Modeling predicts the best word/token \n",
    "in its vocabulary that would replace that word. \n",
    "\n",
    "The logits are the output of the BERT Model before a softmax activation function\n",
    " is applied to the output of BERT. \n",
    "i.e. logits are the Prediction scores of the language modeling head \n",
    "(scores for each vocabulary token before SoftMax).\n",
    "\n",
    "And in order to get the logits, we have to specify \"return_dict = True\" \n",
    "in the parameters when initializing the model, \n",
    "otherwise, the above code will result in a compilation error. \n",
    "\n",
    "\"return_dict\" - If set to True, the model will return a ModelOutput class instead of a plain tuple.\n",
    "\n",
    "'''\n",
    "\n",
    "text = \"The Opera House in Australia is in , \" + tokenizer.mask_token + \" city\"\n",
    "\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "''' In order to get the tensor of softmax values of all the words in BERT’s\n",
    "vocabulary for replacing the mask token, we need to specify the masked token index.\n",
    "\n",
    "And these we can get using torch.where(). And in this particular example \n",
    "I am retrieving the top 10 candidate replacement words for the mask token. '''\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "''' mask_token (str or tokenizers.AddedToken, optional) — A special token \n",
    "representing a masked token (used by masked-language modeling pretraining objectives, like BERT). \n",
    "Will be associated to self.mask_token and self.mask_token_id. '''\n",
    "\n",
    "output = model(**input)\n",
    "\n",
    "logits = output.logits\n",
    "# print(logits.shape) -> (1, 12, 30522)\n",
    "''' After we pass the input encoding into the BERT Model, \n",
    "we can get the logits simply by specifying output.logits, which returns a tensor, \n",
    "and after this we can finally apply a softmax activation function to the logits. '''\n",
    "\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "''' By applying a softmax onto the output of BERT, \n",
    "we get probabilistic distributions for each of the words in BERT’s vocabulary.\n",
    "Word’s with a higher probability value will be better candidate replacement words \n",
    "for the mask token.  '''\n",
    "\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "''' In order to get the tensor of softmax values of all the words in BERT’s vocabulary \n",
    "for replacing the mask token, we can specify the masked token index, \n",
    "which we already got using torch.where(). '''\n",
    "\n",
    "\n",
    "\n",
    "#retrieving the top 10 candidate replacement words for the mask token. \n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "'''torch.topk() retrieves the top k values in a given tensor, \n",
    " and it returns a tensor containing those top k values. '''\n",
    "\n",
    "'''Iterate through the tensor and replace the mask token in the sentence with the candidate token. '''\n",
    "for token in top_10:\n",
    "   word = tokenizer.decode([token])\n",
    "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "   print(new_sentence)\n",
    "\n",
    "top_word = torch.argmax(mask_word, dim=1) #returns token_id\n",
    "print(tokenizer.decode(top_word))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Sentence Prediction\n",
    "Next Sentence Prediction is the task of predicting whether one sentence follows another sentence.\n",
    "\n",
    "BertForNextSentencePrediction\n",
    "It returns logits (torch.FloatTensor of shape (batch_size, 2)) – Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9998e-01, 1.5085e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"I came back from Office in the evening\"\n",
    "\n",
    "next_sentence = \"I opened my Beer after Office\"\n",
    "\n",
    "#BERT tokenizer automatically inserts a [SEP] token in between the sentences\n",
    "encoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "outputs = model(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)\n",
    "\n",
    "#Bert returns two values in a tensor: \n",
    "# the first value represents whether the second sentence is a continuation of the first\n",
    "# second value represents whether the second sentence is a random sequence AKA not a good continuation of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1332,  1225, 15175,  1942,   118,   124,  1435,   102, 15175,\n",
      "          1942,   118,   124,  1338,  1107, 12795,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "#Question Answering\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "\n",
    "example_text = \"GPT-3 came in 2020\"\n",
    "\n",
    "example_question = \"When did GPT-3 come\"\n",
    "\n",
    "# We can use our tokenizer to automatically generate 2 sentence by passing the\n",
    "# two sequences to tokenizer as two arguments\n",
    "tokenized_inputs = tokenizer(example_question, example_text, return_tensors=\"pt\")\n",
    "print(tokenized_inputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert QA appends question before context.\n",
    "\n",
    "Tokenizer returns 3 tensors for us.\n",
    "\n",
    "“inputs_ids” are tokenized ids of text.\n",
    "\"'token_type_ids' => To understand them first note, Some models’ purpose is to do classification on pairs of sentences or question answering.\n",
    "https://huggingface.co/docs/transformers/v4.20.1/en/glossary#token-type-ids\n",
    "\n",
    "These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:\n",
    "\n",
    "[CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n",
    "We used our tokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments\n",
    "\n",
    "BERT has token type IDs (also called segment IDs). They are represented as a binary mask identifying the two types of sequence in the model.\n",
    "\n",
    "Here those 2 types of sequences are Questions and the Context. Token type 0 is for question part and 1 context.\n",
    "\n",
    "The model will tell you at what start and end position of the input_ids the answer to the question will be located.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1475'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\"\n",
    "\n",
    "question = \"When was the Vat formally opened?\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "\n",
    "tokenized_inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "''' start_logits (torch.FloatTensor of shape (batch_size, sequence_length)) \n",
    "— Span-start scores (before SoftMax).\n",
    "\n",
    "end_logits (torch.FloatTensor of shape (batch_size, sequence_length)) \n",
    "— Span-end scores (before SoftMax). '''\n",
    "\n",
    "predict_answer_tokens = tokenized_inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "521bf23a49300457a383cc0ce4a9a5b8cdf2cad9d8aaec6ddd3bd1c99845bf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
